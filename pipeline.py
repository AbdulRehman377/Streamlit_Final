"""
Document Processing Pipeline

Orchestrates the full document processing flow:
1. Azure OCR extraction (azure_ocr.py)
2. Enhanced chunking (enhanced_chunker.py)
3. Store in ChromaDB (store_enhanced_chunks.py)

Usage:
    python pipeline.py                    # Uses default PDF from config
    python pipeline.py invoice.pdf        # Process specific PDF
    python pipeline.py invoice.pdf myDoc  # Process with custom doc ID
"""

import sys
import os
import json
import time

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEFAULT_PDF = "invoice2.PDF"
RAW_OCR_PATH = "RAW_OCR.json"
ENHANCED_CHUNKS_PATH = "ENHANCED_CHUNKS.json"
DEFAULT_DOC_ID = "document"


def run_pipeline(pdf_path: str, doc_id: str = None):
    """
    Run the full document processing pipeline.
    
    Args:
        pdf_path: Path to the PDF file
        doc_id: Document identifier for ChromaDB (defaults to filename without extension)
    """
    start_time = time.time()
    
    # Validate PDF exists
    if not os.path.exists(pdf_path):
        print(f"âŒ Error: PDF file not found: {pdf_path}")
        return False
    
    # Use filename as doc_id if not provided
    if not doc_id:
        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]
    
    print("\n" + "â•" * 60)
    print("ðŸš€ DOCUMENT PROCESSING PIPELINE")
    print("â•" * 60)
    print(f"   PDF:      {pdf_path}")
    print(f"   Doc ID:   {doc_id}")
    print("â•" * 60)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: Azure OCR Extraction
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("ðŸ“„ STEP 1: Azure OCR Extraction")
    print("â”€" * 60)
    
    try:
        from azure_ocr import analyze_layout_rest
        
        result = analyze_layout_rest(pdf_path)
        
        # Save raw OCR output
        with open(RAW_OCR_PATH, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… OCR complete â†’ {RAW_OCR_PATH}")
        
    except Exception as e:
        print(f"âŒ OCR failed: {e}")
        return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: Enhanced Chunking
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("ðŸ“¦ STEP 2: Enhanced Chunking")
    print("â”€" * 60)
    
    try:
        from enhanced_chunker import EnhancedChunker
        
        chunker = EnhancedChunker(config={
            "min_chunk_length": 50,
            "max_chunk_length": 4000,
        })
        
        raw_ocr = chunker.load_raw_ocr(RAW_OCR_PATH)
        chunks = chunker.extract_chunks(raw_ocr, filename=os.path.basename(pdf_path))
        chunker.save_chunks(chunks, ENHANCED_CHUNKS_PATH)
        
        print(f"âœ… Chunking complete â†’ {ENHANCED_CHUNKS_PATH}")
        
    except Exception as e:
        print(f"âŒ Chunking failed: {e}")
        return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: Store in ChromaDB
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "â”€" * 60)
    print("ðŸ—„ï¸  STEP 3: Store in ChromaDB")
    print("â”€" * 60)
    
    try:
        from store_enhanced_chunks import load_enhanced_chunks, store_chunks
        
        chunks = load_enhanced_chunks(ENHANCED_CHUNKS_PATH)
        print(f"   Loaded {len(chunks)} chunks")
        
        vectorstore = store_chunks(chunks, doc_id=doc_id)
        
        print(f"âœ… Storage complete â†’ ChromaDB")
        
    except Exception as e:
        print(f"âŒ Storage failed: {e}")
        return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DONE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    elapsed = time.time() - start_time
    
    print("\n" + "â•" * 60)
    print("âœ… PIPELINE COMPLETE")
    print("â•" * 60)
    print(f"   Total time: {elapsed:.1f}s")
    print(f"   Output files:")
    print(f"     â€¢ {RAW_OCR_PATH}")
    print(f"     â€¢ {ENHANCED_CHUNKS_PATH}")
    print(f"     â€¢ ./chroma_db_enhanced/")
    print("â•" * 60)
    print("\nðŸ’¡ Run 'python rag_chat.py' to query your document!\n")
    
    return True


def main():
    """Main entry point with CLI argument handling."""
    # Parse arguments
    pdf_path = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_PDF
    doc_id = sys.argv[2] if len(sys.argv) > 2 else None
    
    # Run pipeline
    success = run_pipeline(pdf_path, doc_id)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
